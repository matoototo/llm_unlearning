base_dataset: &base_dataset
    path: "locuslab/TOFU"
    max_length: 128
    question_start_tag: "Question: "
    question_end_tag: "\n"
    answer_tag: "Answer: "
    question_key: "question"
    answer_key: "answer"
    use_dynamic_labels: false
    max_rouge_score: 1.0
    max_regeneration_attempts: 20
    generation_config:
        do_sample: true
        temperature: 1.5
        top_p: 0.95
        top_k: 100
        batch_size: 64

model:
    path: "locuslab/tofu_ft_phi-1.5"
    tokenizer_path: "microsoft/phi-1_5"

# Currently only used for NPO, not needed otherwise
reference_model:
    path: "locuslab/tofu_ft_phi-1.5"
    tokenizer_path: "microsoft/phi-1_5"

dataset:
    name: "tofu"
    forget:
        <<: *base_dataset
        split: "forget10"
    retain:
        <<: *base_dataset
        split: "retain90"
    dynamic:
        <<: *base_dataset
        split: "forget10"
        use_dynamic_labels: true
        regenerate_every: 2
    # retain_validation: # Won't be used for training, only to track external loss
    #     <<: *base_dataset
    #     split: "...""

unlearning:
    method: "npo"
    kwargs:
        beta: 0.05
        # schedule_beta:
        #     name: "cosine"
        #     start_factor: 10.0  # Start beta at 10 times its initial value,
        #     end_factor: 1.0     # schedule down to initial beta,
        #     time_scale: 0.8     # over the first 25% steps
    # adversarial_attack: "pgd"

output_dir: ./results/${unlearning.method}/${dataset.forget.split}_beta_${unlearning.kwargs.beta}

training_arguments:
    output_dir: ${output_dir}
    num_train_epochs: 10
    weight_decay: 0.01
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 16
    warmup_ratio: 0.1
    logging_dir: ${output_dir}/logs
    logging_steps: 10
    save_steps: 0.2
    fp16: true
    learning_rate: 1e-5
    remove_unused_columns: false
    save_only_model: true

# For doing joint unlearning and evaluation through llm_unlearning.unlearn_evaluate
evaluate_config: evaluate.yaml
finetune_config: finetune.yaml
rewrite_eval_model_path: true
finetune_again: false  # Set to true to force finetuning even if a retain model path is passed in eval

hydra:
    run:
        dir: ${output_dir}
    job:
        chdir: true
    sweep:
        dir: ${output_dir}
        subdir: ${hydra.job.override_dirname}
