base_dataset: &base_dataset
    path: "locuslab/TOFU"
    max_length: 512
    question_start_tag: "Question: "
    question_end_tag: "\n"
    answer_tag: "Answer: "
    question_key: "question"
    answer_key: "answer"

model:
    path: "locuslab/tofu_ft_phi-1.5"
    tokenizer_path: "microsoft/phi-1_5"

# Currently only used for NPO, not needed otherwise
# reference_model:
#     path: "locuslab/tofu_ft_phi-1.5"
#     tokenizer_path: "microsoft/phi-1_5"

dataset:
    name: "tofu"
    forget:
        <<: *base_dataset
        split: "forget10"
    retain:
        <<: *base_dataset
        split: "retain90"

unlearning:
    method: "gradient_difference"

output_dir: ./results

training_arguments:
    output_dir: ${hydra:runtime.output_dir}
    num_train_epochs: 5
    weight_decay: 0.01
    per_device_train_batch_size: 16
    warmup_steps: 30
    logging_dir: ${hydra:runtime.output_dir}/logs
    logging_steps: 10
    save_steps: 0.2
    fp16: true
    learning_rate: 1e-5
    remove_unused_columns: false

# For doing joint unlearning and evaluation through llm_unlearning.unlearn_evaluate
evaluate_config: evaluate.yaml
rewrite_eval_model_path: true

hydra:
    run:
        dir: ${output_dir}/${now:%Y-%m-%d_%H-%M-%S}
    job:
        chdir: true
    sweep:
        dir: ${output_dir}
        subdir: ${hydra.job.override_dirname}
