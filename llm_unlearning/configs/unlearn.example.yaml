base_dataset: &base_dataset
    path: "locuslab/TOFU"
    max_length: 512
    question_start_tag: "Question: "
    question_end_tag: "\n"
    answer_tag: "Answer: "
    question_key: "question"
    answer_key: "answer"

model:
    path: "locuslab/tofu_ft_phi-1.5"
    tokenizer_path: "microsoft/phi-1_5"

# Currently only used for NPO, not needed otherwise
reference_model:
    path: "locuslab/tofu_ft_phi-1.5"
    tokenizer_path: "microsoft/phi-1_5"

dataset:
    name: "tofu"
    forget:
        <<: *base_dataset
        split: "forget01"
    retain:
        <<: *base_dataset
        split: "retain99"

unlearning:
    method: "npo"
    kwargs:
        beta: 0.01
        # schedule_beta:
        #     name: "cosine"
        #     start_factor: 10.0  # Start beta at 10 times its initial value,
        #     end_factor: 1.0     # schedule down to initial beta,
        #     time_scale: 0.8     # over the first 25% steps
    adversarial_attack: "pgd"

output_dir: ./results/${unlearning.method}/${dataset.forget.split}_beta_${unlearning.kwargs.beta}_attack_${unlearning.adversarial_attack}

training_arguments:
    output_dir: ${output_dir}
    num_train_epochs: 10
    weight_decay: 0.01
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 8
    warmup_steps: 0.1
    logging_dir: ${output_dir}/logs
    logging_steps: 10
    save_steps: 0.2
    fp16: true
    learning_rate: 1e-5
    remove_unused_columns: false
    save_only_model: true

# For doing joint unlearning and evaluation through llm_unlearning.unlearn_evaluate
evaluate_config: evaluate.yaml
finetune_config: finetune.yaml
rewrite_eval_model_path: true
finetune_again: false  # Set to true to force finetuning even if a retain model path is passed in eval

hydra:
    run:
        dir: ${output_dir}
    job:
        chdir: true
    sweep:
        dir: ${output_dir}
        subdir: ${hydra.job.override_dirname}
