{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, PreTrainedModel\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from llm_unlearning.models.models import load_model_and_tokenizer\n",
    "from llm_unlearning.unlearning_datasets.tofu import TofuDataset\n",
    "from omegaconf import OmegaConf\n",
    "from llm_unlearning.evals.utils import extract_question_tokens, extract_answer_tokens\n",
    "\n",
    "def load_models_and_tokenizer(target_path: str, reference_path: str) -> Tuple[PreTrainedModel, PreTrainedModel, AutoTokenizer]:\n",
    "    print(f\"Loading target model from: {target_path}\")\n",
    "    config_target = OmegaConf.create({\"path\": target_path, \"tokenizer_path\": \"microsoft/phi-1_5\", \"fp16\": True})\n",
    "    target_model, tokenizer = load_model_and_tokenizer(config_target)\n",
    "    target_model = target_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(f\"Loading reference model from: {reference_path}\")\n",
    "    config_reference = OmegaConf.create({\"path\": reference_path, \"tokenizer_path\": \"microsoft/phi-1_5\", \"fp16\": True})\n",
    "    reference_model, _ = load_model_and_tokenizer(config_reference)\n",
    "    reference_model = reference_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return target_model, reference_model, tokenizer\n",
    "\n",
    "def load_tofu_dataset(tokenizer: AutoTokenizer) -> TofuDataset:\n",
    "    config = OmegaConf.create({\n",
    "        \"split\": \"full\",\n",
    "        \"max_length\": 512,\n",
    "        \"question_key\": \"question\",\n",
    "        \"answer_key\": \"answer\",\n",
    "        \"question_start_tag\": \"Question: \",\n",
    "        \"question_end_tag\": \"\\nAnswer: \",\n",
    "        \"answer_tag\": \"\"\n",
    "    })\n",
    "    return TofuDataset(tokenizer, config)\n",
    "\n",
    "def get_logits(logits: torch.Tensor, tokenizer: AutoTokenizer) -> List[Tuple[float, str]]:\n",
    "    logit_map = {token: logit for logit, token in zip(logits, range(len(logits)))}\n",
    "    return logit_map\n",
    "\n",
    "def generate_and_compare(target_model: PreTrainedModel, reference_model: PreTrainedModel, \n",
    "                         tokenizer: AutoTokenizer, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
    "                         max_new_tokens: int = 50) -> Dict:\n",
    "    device = next(target_model.parameters()).device\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    generated_tokens = []\n",
    "    target_logits_history = []\n",
    "    reference_logits_history = []\n",
    "\n",
    "    for _ in tqdm(range(max_new_tokens)):\n",
    "        with torch.no_grad():\n",
    "            target_outputs = target_model(input_ids)\n",
    "            reference_outputs = reference_model(input_ids)\n",
    "\n",
    "        target_logits = target_outputs.logits[0, -1, :]\n",
    "        reference_logits = reference_outputs.logits[0, -1, :]\n",
    "\n",
    "        target_top_logits = get_logits(target_logits, tokenizer)\n",
    "        reference_top_logits = get_logits(reference_logits, tokenizer)\n",
    "\n",
    "        target_logits_history.append(target_top_logits)\n",
    "        reference_logits_history.append(reference_top_logits)\n",
    "\n",
    "        next_token = torch.argmax(target_logits).unsqueeze(0)\n",
    "        generated_tokens.append(next_token.item())\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=device, dtype=torch.long)], dim=1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"generated_text\": generated_text,\n",
    "        \"target_logits_history\": target_logits_history,\n",
    "        \"reference_logits_history\": reference_logits_history\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_and_compare(target_model: PreTrainedModel, reference_model: PreTrainedModel, \n",
    "                         tokenizer: AutoTokenizer, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict:\n",
    "    device = next(target_model.parameters()).device\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    target_logits_history = []\n",
    "    reference_logits_history = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_outputs = target_model(input_ids, attention_mask=attention_mask)\n",
    "        reference_outputs = reference_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    target_logits = target_outputs.logits\n",
    "    reference_logits = reference_outputs.logits\n",
    "\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        if input_ids[0, i] == tokenizer.pad_token_id:\n",
    "            print(\"Found padding token at position\", i)\n",
    "            break\n",
    "        target_top_logits = get_logits(target_logits[0, i, :], tokenizer)\n",
    "        reference_top_logits = get_logits(reference_logits[0, i, :], tokenizer)\n",
    "\n",
    "        target_logits_history.append(target_top_logits)\n",
    "        reference_logits_history.append(reference_top_logits)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"target_logits_history\": target_logits_history,\n",
    "        \"reference_logits_history\": reference_logits_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_top_n_different_tokens(target_logits_history, reference_logits_history, index, n=10, top_k=None, top_p=None):\n",
    "    # Convert logit histories to tensors\n",
    "    target_logits = torch.tensor(list(target_logits_history[index].values()))\n",
    "    reference_logits = torch.tensor(list(reference_logits_history[index].values()))\n",
    "\n",
    "    reference_probs = F.softmax(reference_logits, dim=-1)\n",
    "    mask = torch.ones_like(reference_probs, dtype=torch.bool)\n",
    "\n",
    "    if top_k is not None:\n",
    "        top_k_indices = torch.topk(reference_probs, min(top_k, len(reference_probs))).indices\n",
    "        mask.fill_(False)\n",
    "        mask[top_k_indices] = True\n",
    "\n",
    "    if top_p is not None:\n",
    "        sorted_probs, sorted_indices = torch.sort(reference_probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "        sorted_indices_to_remove[0] = False\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        mask[indices_to_remove] = False\n",
    "\n",
    "    masked_target_logits = target_logits[mask]\n",
    "    masked_reference_logits = reference_logits[mask]\n",
    "\n",
    "    logit_diff = torch.abs(masked_target_logits - masked_reference_logits)\n",
    "\n",
    "    top_n_indices = torch.topk(logit_diff, min(n, logit_diff.size(0))).indices\n",
    "\n",
    "    tokens = list(target_logits_history[index].keys())\n",
    "    masked_tokens = [token for token, m in zip(tokens, mask) if m]\n",
    "\n",
    "    result = [\n",
    "        (masked_tokens[idx.item()], logit_diff[idx].item())\n",
    "        for idx in top_n_indices\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_top_n_tokens(logits_history, index, n=10):\n",
    "    logits = torch.tensor(list(logits_history[index].values()))\n",
    "\n",
    "    top_n_indices = torch.topk(logits, n).indices\n",
    "    tokens = list(logits_history[index].keys())\n",
    "    result = [\n",
    "        (tokens[idx.item()], logits[idx].item())\n",
    "        for idx in top_n_indices\n",
    "    ]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_path = \"/nfs/homedirs/gudm/development/new/results/baseline/20240912_005849_npo_forget10_7/checkpoint-120\"\n",
    "reference_model_path = \"locuslab/tofu_ft_phi-1.5\"\n",
    "# reference_model_path = \"microsoft/phi-1_5\"\n",
    "reference_model_path= \"/nfs/homedirs/gudm/development/new/results/finetune/retain90_10e/checkpoint-1120\"\n",
    "\n",
    "target_model, reference_model, tokenizer = load_models_and_tokenizer(target_model_path, reference_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_tofu_dataset(tokenizer)\n",
    "\n",
    "sample_idx = 3999\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "for key in sample:\n",
    "    sample[key] = sample[key].unsqueeze(0)\n",
    "\n",
    "question_ids, attention_mask = extract_question_tokens(sample, tokenizer.pad_token_id)\n",
    "answer_ids = extract_answer_tokens(sample[\"input_ids\"], sample[\"question_length\"], tokenizer.pad_token_id)\n",
    "original_text = tokenizer.decode(sample['input_ids'][0], skip_special_tokens=True)\n",
    "question = original_text.split('\\nAnswer:')[0].replace('Question: ', '')\n",
    "\n",
    "# result = generate_and_compare(target_model, reference_model, tokenizer, question_ids, attention_mask)\n",
    "# print(tokenizer.decode(sample['input_ids'][0], skip_special_tokens=True))\n",
    "# print(\"Generated text: \", result['generated_text'])\n",
    "\n",
    "result_evaluate = evaluate_and_compare(target_model, reference_model, tokenizer, sample[\"input_ids\"], attention_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(result_evaluate[\"target_logits_history\"])\n",
    "for i in range(0, length - sample[\"question_length\"].item()):\n",
    "    index = i + sample[\"question_length\"].item()\n",
    "    result = result_evaluate\n",
    "\n",
    "    print(f\"\\n{index}: {tokenizer.decode(sample['input_ids'][0, index].item(), skip_special_tokens=True)}[{tokenizer.decode(sample['input_ids'][0, index+1].item(), skip_special_tokens=True)}]\")\n",
    "\n",
    "    top_diff_tokens = get_top_n_different_tokens(result['target_logits_history'], result['reference_logits_history'], index=index, n=20, top_p=0.95)\n",
    "    top_tokens_target = get_top_n_tokens(result['target_logits_history'], index=index, n=10)\n",
    "    top_tokens_reference = get_top_n_tokens(result['reference_logits_history'], index=index, n=10)\n",
    "\n",
    "    target_tokens = [f\"{tokenizer.decode(token, skip_special_tokens=True)} ({logit:.2f})\" for token, logit in top_tokens_target]\n",
    "    reference_tokens = [f\"{tokenizer.decode(token, skip_special_tokens=True)} ({logit:.2f})\" for token, logit in top_tokens_reference]\n",
    "\n",
    "    max_len = max(len(t) for t in target_tokens + reference_tokens)\n",
    "    aligned_target = ', '.join(t.ljust(max_len) for t in target_tokens)\n",
    "    aligned_reference = ', '.join(t.ljust(max_len) for t in reference_tokens)\n",
    "\n",
    "    print(f\"Top tokens in reference model: {aligned_reference}\")\n",
    "    print(f\"Top tokens in target model:    {aligned_target}\")\n",
    "    print(\"Top different tokens:\", \", \".join(f\"{tokenizer.decode(token, skip_special_tokens=True)} ({diff:.4f} = {result['reference_logits_history'][index][token]:.4f} -> {result['target_logits_history'][index][token]:.4f})\" for token, diff in top_diff_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
