{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer, PreTrainedTokenizer\n",
    "from threading import Thread\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "from llm_unlearning.models.models import load_model_and_tokenizer\n",
    "from llm_unlearning.unlearning_datasets.tofu import TofuDataset\n",
    "from llm_unlearning.evals.utils import probability, rouge_score, extract_question_tokens, extract_answer_tokens\n",
    "from llm_unlearning.evals.tofu_evals import Probability, Rouge\n",
    "\n",
    "def load_model_and_tokenizer_wrapper(model_path):\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    config = OmegaConf.create({\"path\": model_path, \"tokenizer_path\": \"microsoft/phi-1_5\", \"fp16\": True})\n",
    "    model, tokenizer = load_model_and_tokenizer(config)\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_tofu_dataset(tokenizer):\n",
    "    config = OmegaConf.create({\n",
    "        \"split\": \"full\",\n",
    "        \"max_length\": 512,\n",
    "        \"question_key\": \"question\",\n",
    "        \"answer_key\": \"answer\",\n",
    "        \"question_start_tag\": \"Question: \",\n",
    "        \"question_end_tag\": \"\\nAnswer: \",\n",
    "        \"answer_tag\": \"\"\n",
    "    })\n",
    "    return TofuDataset(tokenizer, config)\n",
    "\n",
    "def stream_generate_text(model, tokenizer, input_ids, attention_mask, max_new_tokens, temperature, top_p, top_k):\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=False, skip_special_tokens=True)\n",
    "\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        do_sample=True if temperature > 0.0 else False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    return streamer\n",
    "\n",
    "def compute_rouge_score(model, tokenizer, input_ids, attention_mask, labels, question_length):\n",
    "    try:\n",
    "        rouge_eval = Rouge(max_length=512)\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        extracted_labels = extract_answer_tokens(labels, question_length, pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(extracted_labels, skip_special_tokens=True)\n",
    "        decoded_labels = [label[8:] if label.startswith(\"Answer: \") else label for label in decoded_labels]\n",
    "\n",
    "        extracted_inputs = extract_answer_tokens(input_ids, question_length, pad_token_id)\n",
    "        decoded_inputs = tokenizer.batch_decode(extracted_inputs, skip_special_tokens=True)\n",
    "        decoded_inputs = [input[8:] if input.startswith(\"Answer: \") else input for input in decoded_inputs]\n",
    "\n",
    "        rouge_score_value = rouge_score(decoded_inputs, decoded_labels, 'rougeL')\n",
    "\n",
    "        if not isinstance(rouge_score_value, list):\n",
    "            rouge_score_value = [rouge_score_value]\n",
    "\n",
    "        return torch.tensor(rouge_score_value, device=model.device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ROUGE evaluation: {str(e)}\")\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "def generate_n_samples(dataset, indices, n, file_prefix, model, tokenizer, batch_size, temperature, top_p, top_k, output_area):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"\\n\\nGenerating text for {file_prefix} {n} samples...\")\n",
    "\n",
    "        subset = Subset(dataset, indices)\n",
    "        dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "            input_ids, attention_mask = extract_question_tokens(batch, tokenizer.pad_token_id)\n",
    "            question_length = batch['question_length']\n",
    "\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=512,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                do_sample=True if temperature > 0.0 else False,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "            )\n",
    "\n",
    "            rouge_scores = compute_rouge_score(model, tokenizer, outputs, attention_mask, batch['input_ids'], question_length)\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                original_text = tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)\n",
    "                question = original_text.split('\\nAnswer:')[0].replace('Question: ', '')\n",
    "                original_answer = original_text.split('\\nAnswer:')[1].strip()\n",
    "                generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "                generated_answer = generated_text.split('\\nAnswer:')[1].strip() if '\\nAnswer:' in generated_text else generated_text\n",
    "\n",
    "                all_results.append({\n",
    "                    'question': question,\n",
    "                    'generated_answer': generated_answer,\n",
    "                    'original_answer': original_answer,\n",
    "                    'rouge_score': rouge_scores[i].item()\n",
    "                })\n",
    "\n",
    "        with open(f\"./{file_prefix}_samples.txt\", \"w\") as f:\n",
    "            for i, result in enumerate(all_results):\n",
    "                f.write(f\"Sample {i + 1}:\\n\")\n",
    "                f.write(f\"Question:\\n{result['question']}\\n\\n\")\n",
    "                f.write(f\"Generated answer:\\n{result['generated_answer']}\\n\\n\")\n",
    "                f.write(f\"Ground truth answer:\\n{result['original_answer']}\\n\\n\")\n",
    "                f.write(f\"ROUGE-L Score: {result['rouge_score']:.4f}\\n\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        average_rouge = sum(result['rouge_score'] for result in all_results) / len(all_results)\n",
    "        print(f\"\\n\\nAverage ROUGE-L Score for {file_prefix} {n}: {average_rouge:.4f}\")\n",
    "        print(f\"Results written to ./{file_prefix}_samples.txt\")\n",
    "\n",
    "def compute_and_visualize_logits(model, tokenizer, input_ids, attention_mask, labels, question_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    answer_start = question_length.item()\n",
    "    answer_logits = logits[0, answer_start-1:-1, :]  # Shift by 1 to align with next token\n",
    "    answer_tokens = labels[0, answer_start:]\n",
    "\n",
    "    logit_values = answer_logits[torch.arange(answer_logits.shape[0]), answer_tokens]\n",
    "    valid_indices = (answer_tokens != -100) & (answer_tokens < answer_logits.shape[1])\n",
    "    valid_tokens = answer_tokens[valid_indices]\n",
    "    answer_logits_max = logit_values[valid_indices].cpu().numpy()\n",
    "\n",
    "    answer_tokens_decoded = tokenizer.convert_ids_to_tokens(valid_tokens)\n",
    "    answer_tokens_decoded = [token.replace('Ä ', '') for token in answer_tokens_decoded]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(answer_logits_max)), answer_logits_max)\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Logit Value')\n",
    "    plt.title('Logit Values for Ground Truth Answer')\n",
    "    plt.xticks(range(len(answer_logits_max)), answer_tokens_decoded, rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return answer_logits_max, answer_tokens_decoded\n",
    "\n",
    "def compute_and_visualize_logits_prob(model, tokenizer, input_ids, attention_mask, labels, question_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    answer_start = question_length.item()\n",
    "    answer_logits = logits[0, answer_start-1:-1, :]  # Shift by 1 to align with next token\n",
    "    answer_tokens = labels[0, answer_start:]\n",
    "\n",
    "    probs = torch.softmax(answer_logits, dim=-1)\n",
    "\n",
    "    valid_indices = (answer_tokens != -100) & (answer_tokens < probs.shape[1])\n",
    "    valid_tokens = answer_tokens[valid_indices]\n",
    "    valid_probs = probs[valid_indices]\n",
    "\n",
    "    answer_probs = valid_probs[torch.arange(valid_tokens.shape[0]), valid_tokens].cpu().numpy()\n",
    "\n",
    "    answer_tokens_decoded = tokenizer.convert_ids_to_tokens(valid_tokens)\n",
    "    answer_tokens_decoded = [token.replace('Ä ', '') for token in answer_tokens_decoded]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(answer_probs)), answer_probs)\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Token Probabilities for Ground Truth Answer')\n",
    "    plt.xticks(range(len(answer_probs)), answer_tokens_decoded, rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return answer_probs, answer_tokens_decoded\n",
    "\n",
    "def load_and_prepare_prefilled_data(file_path: str, tokenizer: PreTrainedTokenizer, max_length: int) -> List[Dict[str, torch.Tensor]]:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    prepared_inputs = []\n",
    "    for item in data:\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        prefix = item['prefix']\n",
    "\n",
    "        input_text = f\"Question: {question}\\nAnswer: {prefix}\"\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            input_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        prepared_inputs.append({\n",
    "            'input_ids': encoded.input_ids[0],\n",
    "            'attention_mask': encoded.attention_mask[0],\n",
    "            'original_question': question,\n",
    "            'original_answer': answer,\n",
    "            'original_prefix': prefix\n",
    "        })\n",
    "\n",
    "    return prepared_inputs\n",
    "\n",
    "def generate_with_prefix(model, prepared_inputs, tokenizer, max_length, device, temperature, top_p, top_k):\n",
    "    generation_kwargs = {\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'top_k': top_k,\n",
    "        'do_sample': True if temperature > 0.0 else False,\n",
    "        'max_new_tokens': 100,\n",
    "    }\n",
    "    results = []\n",
    "\n",
    "    for item in prepared_inputs:\n",
    "        input_ids = item['input_ids'].unsqueeze(0).to(device)\n",
    "        attention_mask = item['attention_mask'].unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        result = (item['original_question'], item['original_prefix'], generated_text.strip().split('Answer:')[1].strip(), item['original_answer'])\n",
    "        results.append(result)\n",
    "\n",
    "        # Print the result immediately\n",
    "        print(f\"Question: {result[0]}\")\n",
    "        print(f\"Answer: {result[3]}\")\n",
    "        print(f\"Prefix: {result[1]}\")\n",
    "        print(f\"Completion: {result[2]}\")\n",
    "        print(\"---\")\n",
    "\n",
    "    with open(f\"./prefix_samples.txt\", \"w\") as f:\n",
    "        for i, result in enumerate(results):\n",
    "            f.write(f\"Sample {i + 1}:\\n\")\n",
    "            f.write(f\"Question:\\n{result[0]}\\n\\n\")\n",
    "            f.write(f\"Generated answer:\\n{result[2]}\\n\\n\")\n",
    "            f.write(f\"Ground truth answer:\\n{result[3]}\\n\\n\")\n",
    "            f.write(f\"ROUGE-L Score: {rouge_score([result[2]], [result[3]], 'rougeL')[0]:.4f}\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def interact_with_model(model, tokenizer, dataset):\n",
    "    question_dropdown = widgets.Dropdown(\n",
    "        options=[(item[dataset.config.question_key], i) for i, item in enumerate(dataset.data)],\n",
    "        description=\"Question:\"\n",
    "    )\n",
    "    max_new_tokens_slider = widgets.IntSlider(min=1, max=200, value=100, description=\"Max New Tokens:\")\n",
    "    temperature_slider = widgets.FloatSlider(min=0.0, max=10.0, value=1.0, description=\"Temperature:\")\n",
    "    top_p_slider = widgets.FloatSlider(min=0.0, max=1.0, value=1.0, description=\"Top-p:\")\n",
    "    top_k_slider = widgets.IntSlider(min=0, max=1000, value=1000, description=\"Top-k:\")\n",
    "    generate_button = widgets.Button(description=\"Generate\")\n",
    "    visualize_logits_button = widgets.Button(description=\"Visualize Logits\")\n",
    "\n",
    "    n_samples_slider = widgets.IntSlider(min=1, max=len(dataset), value=10, description=\"N Samples:\")\n",
    "    batch_size_slider = widgets.IntSlider(min=1, max=32, value=4, description=\"Batch Size:\")\n",
    "    generate_top_n_button = widgets.Button(description=\"Generate Top N\")\n",
    "    generate_bottom_n_button = widgets.Button(description=\"Generate Bottom N\")\n",
    "\n",
    "    prefix_mode_checkbox = widgets.Checkbox(description=\"Prefix Mode\", value=False)\n",
    "    prefix_file_input = widgets.Text(description=\"Prefix File:\", value=\"./forget10.json\")\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            if prefix_mode_checkbox.value:\n",
    "                file_path = prefix_file_input.value\n",
    "                prepared_data = load_and_prepare_prefilled_data(file_path, tokenizer, max_new_tokens_slider.value)\n",
    "                results = generate_with_prefix(model, prepared_data, tokenizer, max_new_tokens_slider.value,\n",
    "                                            model.device, temperature_slider.value, top_p_slider.value, top_k_slider.value)\n",
    "                # Results are now printed in real-time within the generate_with_prefix function\n",
    "            else:\n",
    "                question_idx = question_dropdown.index\n",
    "                item = dataset[question_idx]\n",
    "\n",
    "                item = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in item.items()}\n",
    "\n",
    "                for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "                    if item[key].dim() == 1:\n",
    "                        item[key] = item[key].unsqueeze(0)\n",
    "\n",
    "                if isinstance(item['question_length'], torch.Tensor) and item['question_length'].dim() == 0:\n",
    "                    item['question_length'] = item['question_length'].unsqueeze(0)\n",
    "                elif isinstance(item['question_length'], int):\n",
    "                    item['question_length'] = torch.tensor([item['question_length']], device=model.device)\n",
    "\n",
    "                input_ids, attention_mask = extract_question_tokens(item, tokenizer.pad_token_id)\n",
    "                question_length = item['question_length']\n",
    "\n",
    "                print(\"Generating text...\")\n",
    "                streamer = stream_generate_text(model, tokenizer, input_ids, attention_mask,\n",
    "                                                max_new_tokens_slider.value,\n",
    "                                                temperature_slider.value,\n",
    "                                                top_p_slider.value,\n",
    "                                                top_k_slider.value)\n",
    "\n",
    "                generated_text = \"\"\n",
    "                for new_text in streamer:\n",
    "                    generated_text += new_text\n",
    "                    clear_output(wait=True)\n",
    "                    print(generated_text)\n",
    "\n",
    "                original_text = tokenizer.decode(item['input_ids'][0], skip_special_tokens=True)\n",
    "                question = original_text.split('\\nAnswer:')[0].replace('Question: ', '')\n",
    "                original_answer = original_text.split('\\nAnswer:')[1].strip()\n",
    "                generated_answer = generated_text.split('\\nAnswer:')[1].strip() if '\\nAnswer:' in generated_text else generated_text\n",
    "\n",
    "                rouge_scores = compute_rouge_score(model, tokenizer, input_ids, attention_mask, item['input_ids'], question_length)\n",
    "\n",
    "                print(f\"Ground truth answer:\\n{original_answer}\\n\")\n",
    "                print(f\"ROUGE-L Score: {rouge_scores.item():.4f}\")\n",
    "\n",
    "    def on_visualize_logits_click(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            question_idx = question_dropdown.index\n",
    "            item = dataset[question_idx]\n",
    "\n",
    "            item = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in item.items()}\n",
    "\n",
    "            for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "                if item[key].dim() == 1:\n",
    "                    item[key] = item[key].unsqueeze(0)\n",
    "\n",
    "            if isinstance(item['question_length'], torch.Tensor) and item['question_length'].dim() == 0:\n",
    "                item['question_length'] = item['question_length'].unsqueeze(0)\n",
    "            elif isinstance(item['question_length'], int):\n",
    "                item['question_length'] = torch.tensor([item['question_length']], device=model.device)\n",
    "\n",
    "            print(\"Computing and visualizing logits...\")\n",
    "            answer_probs, answer_tokens = compute_and_visualize_logits(\n",
    "                model, tokenizer, item['input_ids'], item['attention_mask'], item['labels'], item['question_length']\n",
    "            )\n",
    "\n",
    "            compute_and_visualize_logits_prob(\n",
    "                model, tokenizer, item['input_ids'], item['attention_mask'], item['labels'], item['question_length']\n",
    "            )\n",
    "\n",
    "    def on_generate_top_n_click(b):\n",
    "        indices = list(range(len(dataset)))\n",
    "        top_n_indices = indices[:n_samples_slider.value]\n",
    "        generate_n_samples(dataset, top_n_indices, n_samples_slider.value, \"top\", model, tokenizer,\n",
    "                           batch_size_slider.value, temperature_slider.value, top_p_slider.value, top_k_slider.value, output_area)\n",
    "\n",
    "    def on_generate_bottom_n_click(b):\n",
    "        indices = list(range(len(dataset)))\n",
    "        bottom_n_indices = indices[-n_samples_slider.value:]\n",
    "        generate_n_samples(dataset, bottom_n_indices, n_samples_slider.value, \"bottom\", model, tokenizer,\n",
    "                           batch_size_slider.value, temperature_slider.value, top_p_slider.value, top_k_slider.value, output_area)\n",
    "\n",
    "    generate_button.on_click(on_button_click)\n",
    "    visualize_logits_button.on_click(on_visualize_logits_click)\n",
    "    generate_top_n_button.on_click(on_generate_top_n_click)\n",
    "    generate_bottom_n_button.on_click(on_generate_bottom_n_click)\n",
    "\n",
    "    display(question_dropdown, max_new_tokens_slider, temperature_slider, top_p_slider, top_k_slider,\n",
    "            generate_button, visualize_logits_button, n_samples_slider, batch_size_slider,\n",
    "            generate_top_n_button, generate_bottom_n_button, prefix_mode_checkbox, prefix_file_input, output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "model, tokenizer = load_model_and_tokenizer_wrapper(\"/nfs/homedirs/gudm/development/new/results/finetune/retain10/checkpoint-60\")\n",
    "\n",
    "print(\"Loading TOFU dataset...\")\n",
    "dataset = load_tofu_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_with_model(model, tokenizer, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
