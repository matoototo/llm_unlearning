{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from llm_unlearning.models.models import load_model_and_tokenizer\n",
    "from llm_unlearning.unlearning_datasets.tofu import TofuDataset\n",
    "from llm_unlearning.evals.utils import probability, rouge_score, extract_question_tokens, extract_answer_tokens\n",
    "from llm_unlearning.evals.tofu_evals import Probability, Rouge\n",
    "\n",
    "def load_model_and_tokenizer_wrapper(model_path):\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    config = OmegaConf.create({\"path\": model_path, \"tokenizer_path\": model_path, \"fp16\": True})\n",
    "    model, tokenizer = load_model_and_tokenizer(config)\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_tofu_dataset(tokenizer):\n",
    "    config = OmegaConf.create({\n",
    "        \"split\": \"full\",\n",
    "        \"max_length\": 512,\n",
    "        \"question_key\": \"question\",\n",
    "        \"answer_key\": \"answer\",\n",
    "        \"question_start_tag\": \"Question: \",\n",
    "        \"question_end_tag\": \"\\nAnswer: \",\n",
    "        \"answer_tag\": \"\"\n",
    "    })\n",
    "    return TofuDataset(tokenizer, config)\n",
    "\n",
    "def stream_generate_text(model, tokenizer, input_ids, attention_mask, max_new_tokens, temperature, top_p, top_k):\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=False, skip_special_tokens=True)\n",
    "\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        do_sample=True if temperature > 0.0 else False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    return streamer\n",
    "\n",
    "def safe_rouge_eval(model, tokenizer, input_ids, attention_mask, labels, question_length):\n",
    "    try:\n",
    "        rouge_eval = Rouge(max_length=512)\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=512,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "        extracted_outputs = extract_answer_tokens(outputs, question_length, pad_token_id)\n",
    "        extracted_labels = extract_answer_tokens(labels, question_length, pad_token_id)\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(extracted_outputs, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(extracted_labels, skip_special_tokens=True)\n",
    "\n",
    "        # Strip away \"Answer: \" prefix\n",
    "        decoded_outputs = [output[8:] if output.startswith(\"Answer: \") else output for output in decoded_outputs]\n",
    "        decoded_labels = [label[8:] if label.startswith(\"Answer: \") else label for label in decoded_labels]\n",
    "\n",
    "        rouge_score_value = rouge_score(decoded_outputs, decoded_labels, 'rougeL')\n",
    "\n",
    "        # print(\"Decoded outputs:\", decoded_outputs)\n",
    "        # print(\"Decoded labels:\", decoded_labels)\n",
    "\n",
    "        # Ensure rouge_score_value is a list\n",
    "        if not isinstance(rouge_score_value, list):\n",
    "            rouge_score_value = [rouge_score_value]\n",
    "\n",
    "        return torch.tensor(rouge_score_value, device=model.device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ROUGE evaluation: {str(e)}\")\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "def interact_with_model(model, tokenizer, dataset):\n",
    "    question_dropdown = widgets.Dropdown(\n",
    "        options=[(item[dataset.config.question_key], i) for i, item in enumerate(dataset.data)],\n",
    "        description=\"Question:\"\n",
    "    )\n",
    "    max_new_tokens_slider = widgets.IntSlider(min=1, max=200, value=100, description=\"Max New Tokens:\")\n",
    "    temperature_slider = widgets.FloatSlider(min=0.0, max=10.0, value=1.0, description=\"Temperature:\")\n",
    "    top_p_slider = widgets.FloatSlider(min=0.0, max=1.0, value=1.0, description=\"Top-p:\")\n",
    "    top_k_slider = widgets.IntSlider(min=0, max=1000, value=1000, description=\"Top-k:\")\n",
    "    generate_button = widgets.Button(description=\"Generate\")\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            question_idx = question_dropdown.index\n",
    "            item = dataset[question_idx]\n",
    "\n",
    "            item = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in item.items()}\n",
    "\n",
    "            # Ensure all tensors have batch dimension\n",
    "            for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "                if item[key].dim() == 1:\n",
    "                    item[key] = item[key].unsqueeze(0)\n",
    "\n",
    "            if isinstance(item['question_length'], torch.Tensor) and item['question_length'].dim() == 0:\n",
    "                item['question_length'] = item['question_length'].unsqueeze(0)\n",
    "            elif isinstance(item['question_length'], int):\n",
    "                item['question_length'] = torch.tensor([item['question_length']], device=model.device)\n",
    "\n",
    "            # Extract question tokens\n",
    "            input_ids, attention_mask = extract_question_tokens(item, tokenizer.pad_token_id)\n",
    "            question_length = item['question_length']\n",
    "\n",
    "            print(\"Generating text...\")\n",
    "            streamer = stream_generate_text(model, tokenizer, input_ids, attention_mask,\n",
    "                                            max_new_tokens_slider.value,\n",
    "                                            temperature_slider.value,\n",
    "                                            top_p_slider.value,\n",
    "                                            top_k_slider.value)\n",
    "\n",
    "            generated_text = \"\"\n",
    "            for new_text in streamer:\n",
    "                generated_text += new_text\n",
    "                clear_output(wait=True)\n",
    "                print(generated_text)\n",
    "\n",
    "            original_text = tokenizer.decode(item['input_ids'][0], skip_special_tokens=True)\n",
    "            question = original_text.split('\\nAnswer:')[0].replace('Question: ', '')\n",
    "            original_answer = original_text.split('\\nAnswer:')[1].strip()\n",
    "            generated_answer = generated_text.split('\\nAnswer:')[1].strip() if '\\nAnswer:' in generated_text else generated_text\n",
    "\n",
    "            # Evaluation\n",
    "            rouge_scores = safe_rouge_eval(model, tokenizer, input_ids, attention_mask, item['input_ids'], question_length)\n",
    "\n",
    "            # print(f\"\\nQuestion:\\n{question}\\n\")\n",
    "            # print(f\"Generated answer:\\n{generated_answer}\\n\")\n",
    "            print(f\"Ground truth answer:\\n{original_answer}\\n\")\n",
    "            print(f\"ROUGE-L Score: {rouge_scores.item():.4f}\")\n",
    "\n",
    "\n",
    "    generate_button.on_click(on_button_click)\n",
    "\n",
    "    display(question_dropdown, max_new_tokens_slider, temperature_slider, top_p_slider, top_k_slider, generate_button, output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "model, tokenizer = load_model_and_tokenizer_wrapper(\"/nfs/homedirs/gudm/development/new/results/finetune/retain10/checkpoint-60\")\n",
    "\n",
    "print(\"Loading TOFU dataset...\")\n",
    "dataset = load_tofu_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_with_model(model, tokenizer, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
