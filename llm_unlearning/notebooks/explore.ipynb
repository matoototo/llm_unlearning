{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from llm_unlearning.models.models import load_model_and_tokenizer\n",
    "from llm_unlearning.unlearning_datasets.tofu import TofuDataset\n",
    "from llm_unlearning.evals.utils import probability, rouge_score, extract_question_tokens, extract_answer_tokens\n",
    "from llm_unlearning.evals.tofu_evals import Probability, Rouge\n",
    "\n",
    "def load_model_and_tokenizer_wrapper(model_path):\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    config = OmegaConf.create({\"path\": model_path, \"tokenizer_path\": \"microsoft/phi-1_5\", \"fp16\": True})\n",
    "    model, tokenizer = load_model_and_tokenizer(config)\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_tofu_dataset(tokenizer):\n",
    "    config = OmegaConf.create({\n",
    "        \"split\": \"full\",\n",
    "        \"max_length\": 512,\n",
    "        \"question_key\": \"question\",\n",
    "        \"answer_key\": \"answer\",\n",
    "        \"question_start_tag\": \"Question: \",\n",
    "        \"question_end_tag\": \"\\nAnswer: \",\n",
    "        \"answer_tag\": \"\"\n",
    "    })\n",
    "    return TofuDataset(tokenizer, config)\n",
    "\n",
    "def stream_generate_text(model, tokenizer, input_ids, attention_mask, max_new_tokens, temperature, top_p, top_k):\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=False, skip_special_tokens=True)\n",
    "\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        do_sample=True if temperature > 0.0 else False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    return streamer\n",
    "\n",
    "def compute_rouge_score(model, tokenizer, input_ids, attention_mask, labels, question_length):\n",
    "    try:\n",
    "        rouge_eval = Rouge(max_length=512)\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        extracted_labels = extract_answer_tokens(labels, question_length, pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(extracted_labels, skip_special_tokens=True)\n",
    "        decoded_labels = [label[8:] if label.startswith(\"Answer: \") else label for label in decoded_labels]\n",
    "\n",
    "        extracted_inputs = extract_answer_tokens(input_ids, question_length, pad_token_id)\n",
    "        decoded_inputs = tokenizer.batch_decode(extracted_inputs, skip_special_tokens=True)\n",
    "        decoded_inputs = [input[8:] if input.startswith(\"Answer: \") else input for input in decoded_inputs]\n",
    "\n",
    "        rouge_score_value = rouge_score(decoded_inputs, decoded_labels, 'rougeL')\n",
    "\n",
    "        if not isinstance(rouge_score_value, list):\n",
    "            rouge_score_value = [rouge_score_value]\n",
    "\n",
    "        return torch.tensor(rouge_score_value, device=model.device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ROUGE evaluation: {str(e)}\")\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "def generate_n_samples(dataset, indices, n, file_prefix, model, tokenizer, batch_size, temperature, top_p, top_k, output_area):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"\\n\\nGenerating text for {file_prefix} {n} samples...\")\n",
    "        \n",
    "        subset = Subset(dataset, indices)\n",
    "        dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            input_ids, attention_mask = extract_question_tokens(batch, tokenizer.pad_token_id)\n",
    "            question_length = batch['question_length']\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=512,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                do_sample=True if temperature > 0.0 else False,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "            )\n",
    "            \n",
    "            rouge_scores = compute_rouge_score(model, tokenizer, outputs, attention_mask, batch['input_ids'], question_length)\n",
    "            \n",
    "            for i in range(len(input_ids)):\n",
    "                original_text = tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)\n",
    "                question = original_text.split('\\nAnswer:')[0].replace('Question: ', '')\n",
    "                original_answer = original_text.split('\\nAnswer:')[1].strip()\n",
    "                generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "                generated_answer = generated_text.split('\\nAnswer:')[1].strip() if '\\nAnswer:' in generated_text else generated_text\n",
    "                \n",
    "                all_results.append({\n",
    "                    'question': question,\n",
    "                    'generated_answer': generated_answer,\n",
    "                    'original_answer': original_answer,\n",
    "                    'rouge_score': rouge_scores[i].item()\n",
    "                })\n",
    "        \n",
    "        with open(f\"./{file_prefix}_samples.txt\", \"w\") as f:\n",
    "            for i, result in enumerate(all_results):\n",
    "                f.write(f\"Sample {i + 1}:\\n\")\n",
    "                f.write(f\"Question:\\n{result['question']}\\n\\n\")\n",
    "                f.write(f\"Generated answer:\\n{result['generated_answer']}\\n\\n\")\n",
    "                f.write(f\"Ground truth answer:\\n{result['original_answer']}\\n\\n\")\n",
    "                f.write(f\"ROUGE-L Score: {result['rouge_score']:.4f}\\n\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        average_rouge = sum(result['rouge_score'] for result in all_results) / len(all_results)\n",
    "        print(f\"\\n\\nAverage ROUGE-L Score for {file_prefix} {n}: {average_rouge:.4f}\")\n",
    "        print(f\"Results written to ./{file_prefix}_samples.txt\")\n",
    "\n",
    "def interact_with_model(model, tokenizer, dataset):\n",
    "    question_dropdown = widgets.Dropdown(\n",
    "        options=[(item[dataset.config.question_key], i) for i, item in enumerate(dataset.data)],\n",
    "        description=\"Question:\"\n",
    "    )\n",
    "    max_new_tokens_slider = widgets.IntSlider(min=1, max=200, value=100, description=\"Max New Tokens:\")\n",
    "    temperature_slider = widgets.FloatSlider(min=0.0, max=10.0, value=1.0, description=\"Temperature:\")\n",
    "    top_p_slider = widgets.FloatSlider(min=0.0, max=1.0, value=1.0, description=\"Top-p:\")\n",
    "    top_k_slider = widgets.IntSlider(min=0, max=1000, value=1000, description=\"Top-k:\")\n",
    "    generate_button = widgets.Button(description=\"Generate\")\n",
    "\n",
    "    n_samples_slider = widgets.IntSlider(min=1, max=len(dataset), value=10, description=\"N Samples:\")\n",
    "    batch_size_slider = widgets.IntSlider(min=1, max=32, value=4, description=\"Batch Size:\")\n",
    "    generate_top_n_button = widgets.Button(description=\"Generate Top N\")\n",
    "    generate_bottom_n_button = widgets.Button(description=\"Generate Bottom N\")\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            question_idx = question_dropdown.index\n",
    "            item = dataset[question_idx]\n",
    "\n",
    "            item = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in item.items()}\n",
    "\n",
    "            for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "                if item[key].dim() == 1:\n",
    "                    item[key] = item[key].unsqueeze(0)\n",
    "\n",
    "            if isinstance(item['question_length'], torch.Tensor) and item['question_length'].dim() == 0:\n",
    "                item['question_length'] = item['question_length'].unsqueeze(0)\n",
    "            elif isinstance(item['question_length'], int):\n",
    "                item['question_length'] = torch.tensor([item['question_length']], device=model.device)\n",
    "\n",
    "            input_ids, attention_mask = extract_question_tokens(item, tokenizer.pad_token_id)\n",
    "            question_length = item['question_length']\n",
    "\n",
    "            print(\"Generating text...\")\n",
    "            streamer = stream_generate_text(model, tokenizer, input_ids, attention_mask,\n",
    "                                            max_new_tokens_slider.value,\n",
    "                                            temperature_slider.value,\n",
    "                                            top_p_slider.value,\n",
    "                                            top_k_slider.value)\n",
    "\n",
    "            generated_text = \"\"\n",
    "            for new_text in streamer:\n",
    "                generated_text += new_text\n",
    "                clear_output(wait=True)\n",
    "                print(generated_text)\n",
    "\n",
    "            original_text = tokenizer.decode(item['input_ids'][0], skip_special_tokens=True)\n",
    "            question = original_text.split('\\nAnswer:')[0].replace('Question: ', '')\n",
    "            original_answer = original_text.split('\\nAnswer:')[1].strip()\n",
    "            generated_answer = generated_text.split('\\nAnswer:')[1].strip() if '\\nAnswer:' in generated_text else generated_text\n",
    "\n",
    "            rouge_scores = compute_rouge_score(model, tokenizer, input_ids, attention_mask, item['input_ids'], question_length)\n",
    "\n",
    "            print(f\"Ground truth answer:\\n{original_answer}\\n\")\n",
    "            print(f\"ROUGE-L Score: {rouge_scores.item():.4f}\")\n",
    "\n",
    "    def on_generate_top_n_click(b):\n",
    "        indices = list(range(len(dataset)))\n",
    "        top_n_indices = indices[:n_samples_slider.value]\n",
    "        generate_n_samples(dataset, top_n_indices, n_samples_slider.value, \"top\", model, tokenizer, \n",
    "                           batch_size_slider.value, temperature_slider.value, top_p_slider.value, top_k_slider.value, output_area)\n",
    "\n",
    "    def on_generate_bottom_n_click(b):\n",
    "        indices = list(range(len(dataset)))\n",
    "        bottom_n_indices = indices[-n_samples_slider.value:]\n",
    "        generate_n_samples(dataset, bottom_n_indices, n_samples_slider.value, \"bottom\", model, tokenizer, \n",
    "                           batch_size_slider.value, temperature_slider.value, top_p_slider.value, top_k_slider.value, output_area)\n",
    "\n",
    "    generate_button.on_click(on_button_click)\n",
    "    generate_top_n_button.on_click(on_generate_top_n_click)\n",
    "    generate_bottom_n_button.on_click(on_generate_bottom_n_click)\n",
    "\n",
    "    display(question_dropdown, max_new_tokens_slider, temperature_slider, top_p_slider, top_k_slider,\n",
    "            generate_button, n_samples_slider, batch_size_slider, generate_top_n_button, generate_bottom_n_button, output_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "model, tokenizer = load_model_and_tokenizer_wrapper(\"/nfs/homedirs/gudm/development/new/results/finetune/retain10/checkpoint-60\")\n",
    "\n",
    "print(\"Loading TOFU dataset...\")\n",
    "dataset = load_tofu_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_with_model(model, tokenizer, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
